{{ $values              := .Values }}
{{ $daemonsetSelector   := include "prometheus-alerts.daemonsetSelector" . }}
{{ $deploymentSelector  := include "prometheus-alerts.deploymentSelector" . }}
{{ $namespaceSelector   := include "prometheus-alerts.namespaceSelector" . }}
{{ $podSelector         := include "prometheus-alerts.podSelector" . }}
{{ $jobSelector         := include "prometheus-alerts.jobSelector" . }}
{{ $hpaSelector         := include "prometheus-alerts.hpaSelector" . }}
{{ $statefulsetSelector := include "prometheus-alerts.statefulsetSelector" . }}

{{- if .Values.containerRules.enabled }}
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: {{ include "prometheus-alerts.fullname" . }}-container-rules
  annotations:
    nextdoor.com/chart: {{ .Values.chart_name }}
    nextdoor.com/source: {{ .Values.chart_source }}
  labels:
    {{- include "nd-common.labels" . | nindent 4 }}
spec:
  groups:
  - name: {{ .Release.Name }}.{{ .Release.Namespace }}.containerRules
    rules:
    {{ with .Values.containerRules.PodContainerTerminated -}}
    - alert: PodContainerTerminated
      annotations:
        summary: {{`Pod {{$labels.pod}} in namespace {{$labels.namespace}} in error status`}}
        runbook_url: {{ $values.defaults.runbookUrl }}#kube-pod-container-terminated
        description: >-
          Pod {{`{{$labels.pod}}`}} in namespace {{`{{$labels.namespace}}`}}
          has a container that has been terminated ({{`{{ $value }}`}} times) due to
          {{`{{$labels.reason}}`}} in the last {{ .for }}.
      expr: |-
        sum by (container, instance, namespace, pod, reason) (
          sum_over_time(
            kube_pod_container_status_terminated_reason{
              reason=~"{{ join "|" .reasons }}",
              {{ $podSelector }},
              {{ $namespaceSelector }}
            }[{{ .over }}]
          )
        ) > {{ .threshold }}
      for: {{ .for }}
      labels:
        severity: {{ .severity }}
        {{- if $values.defaults.additionalRuleLabels }}
        {{ toYaml $values.defaults.additionalRuleLabels | nindent 8 }}
        {{- end }}
    {{ end -}}

    {{ with .Values.containerRules.PodContainerOOMKilled -}}
    - alert: PodContainerOOMKilled
      annotations:
        summary: {{`Pod {{$labels.pod}} in namespace {{$labels.namespace}} in error status`}}
        runbook_url: {{ $values.defaults.runbookUrl }}#kube-pod-container-terminated
        description: >-
          Pod {{`{{$labels.pod}}`}} in namespace {{`{{$labels.namespace}}`}}
          has a container that has been terminated ({{`{{ $value }}`}} times) due to
          {{`{{$labels.reason}}`}} in the last {{ .for }}.
      expr: |-
        sum by (container, instance, namespace, pod, reason) (
          sum_over_time(
            kube_pod_container_status_terminated_reason{
              reason=~"OOMKilled",
              {{ $podSelector }},
              {{ $namespaceSelector }}
            }[{{ .over }}]
          )
        ) > {{ .threshold }}
      for: {{ .for }}
      labels:
        severity: {{ .severity }}
        {{- if $values.defaults.additionalRuleLabels }}
        {{ toYaml $values.defaults.additionalRuleLabels | nindent 8 }}
        {{- end }}
    {{ end -}}

    {{ with .Values.containerRules.ContainerWaiting -}}
    - alert: ContainerWaiting
      annotations:
        summary: Pod container waiting longer than {{ .for }}
        runbook_url: {{ $values.defaults.runbookUrl }}#alert-name-kubecontainerwaiting
        description: >-
          Pod {{`{{`}} $labels.namespace {{`}}`}}/{{`{{`}} $labels.pod {{`}}`}}
          container {{`{{`}} $labels.container{{`}}`}} has been in waiting
          state for longer than 1 hour.
      expr: |-
        sum by (namespace, pod, container) (
          kube_pod_container_status_waiting_reason{ {{- $podSelector -}} }
        ) > 0
      for: {{ .for }}
      labels:
        severity: {{ .severity }}
        {{- if $values.defaults.additionalRuleLabels }}
        {{ toYaml $values.defaults.additionalRuleLabels | nindent 8 }}
        {{- end }}
    {{- end }}

    {{- with .Values.containerRules.CPUThrottlingHigh }}
    - alert: CPUThrottlingHigh
      annotations:
        summary: Processes experience elevated CPU throttling.
        runbook_url: {{ $values.defaults.runbookUrl }}#CPUThrottlingHigh
        description: >-
          {{`{{ $value | humanizePercentage }} throttling of CPU in
          namespace {{ $labels.namespace }} for container {{ $labels.container
          }} in pod {{ $labels.pod }}.`}}
      expr: |-
        sum(increase(container_cpu_cfs_throttled_periods_total{
          container!="",
          {{ $podSelector }},
          {{ $namespaceSelector }}
        }[5m])) by (container, pod, namespace)
          /
        sum(increase(container_cpu_cfs_periods_total{}[5m])) by (container, pod, namespace)
          > ( {{ .threshold }} / 100 )
      for: {{ .for }}
      labels:
        severity: {{ .severity }}
        {{- if $values.defaults.additionalRuleLabels }}
        {{ toYaml $values.defaults.additionalRuleLabels | nindent 8 }}
        {{- end }}
    {{- end }}

  #
  # Original Source:
  #    https://raw.githubusercontent.com/prometheus-community/helm-charts/kube-prometheus-stack-13.3.0/charts/kube-prometheus-stack/templates/prometheus/rules-1.14/kubernetes-apps.yaml
  #
  # This file has been modified so that the individual alarms are configurable.
  # The default values for thresholds, periods and severities made these alarms
  # too limited for us.
  #
  - name: {{ .Release.Name }}.{{ .Release.Namespace }}.kubernetesAppsRules
    rules:

    {{ with .Values.containerRules.PodCrashLoopBackOff -}}
    - alert: PodCrashLoopBackOff
      annotations:
        summary: {{`Container inside pod {{ $labels.pod }} is crash looping`}}
        runbook_url: {{ $values.defaults.runbookUrl }}#alert-name-kubepodcrashlooping
        description: >-
          {{`Container {{ $labels.container }} within pod {{ $labels.pod }} is in
          a {{ $labels.reason }} state. Investigate because it indicates that
          the Pod is unable to become fully healthy.`}}
      expr: |-
        sum by(namespace, pod, container, reason) (
          kube_pod_container_status_waiting_reason{
            job="kube-state-metrics",
            reason="CrashLoopBackOff",
            {{ $podSelector }},
            {{ $namespaceSelector }}
          }
        ) > 0
      for: {{ .for }}
      labels:
        severity: {{ .severity }}
        {{- if $values.defaults.additionalRuleLabels }}
        {{ toYaml $values.defaults.additionalRuleLabels | nindent 8 }}
        {{- end }}
    {{- end }}

    {{ with .Values.containerRules.PodNotReady -}}
    - alert: PodNotReady
      annotations:
        summary: Pod has been in a non-ready state for more than {{ .for }}
        runbook_url: {{ $values.defaults.runbookUrl }}#alert-name-kubepodnotready
        description: >-
          Pod {{`{{`}} $labels.namespace {{`}}`}}/{{`{{`}} $labels.pod {{`}}`}}
          has been in a non-ready state for longer than {{ .for }}.
      expr: |-
        sum by (namespace, pod) (
          max by(namespace, pod) (
            kube_pod_status_phase{
              job="kube-state-metrics",
              phase=~"Pending|Unknown",
              {{ $podSelector }},
              {{ $namespaceSelector }}
            }
          )
            *
          on(namespace, pod)
          group_left(owner_kind)
          topk by(namespace, pod) (
            1,
            max by(namespace, pod, owner_kind) (kube_pod_owner{owner_kind!="Job"})
          )
        ) > 0
      for: {{ .for }}
      labels:
        severity: {{ .severity }}
        {{- if $values.defaults.additionalRuleLabels }}
        {{ toYaml $values.defaults.additionalRuleLabels | nindent 8 }}
        {{- end }}
    {{- end }}

    {{ with .Values.containerRules.KubeDeploymentGenerationMismatch -}}
    - alert: KubeDeploymentGenerationMismatch
      annotations:
        summary: Deployment generation mismatch due to possible roll-back
        runbook_url: {{ $values.defaults.runbookUrl }}#alert-name-kubedeploymentgenerationmismatch
        description: >-
          Deployment generation for {{`{{`}} $labels.namespace
          {{`}}`}}/{{`{{`}} $labels.deployment {{`}}`}} does not match, this
          indicates that the Deployment has failed but has not been rolled
          back.
      expr: |-
        kube_deployment_status_observed_generation{ {{- $deploymentSelector -}} }
          !=
        kube_deployment_metadata_generation{ {{- $deploymentSelector -}} }
      for: {{ .for }}
      labels:
        severity: {{ .severity }}
        {{- if $values.defaults.additionalRuleLabels }}
        {{ toYaml $values.defaults.additionalRuleLabels | nindent 8 }}
        {{- end }}
    {{- end }}

    {{ with .Values.containerRules.KubeStatefulSetReplicasMismatch -}}
    - alert: KubeStatefulSetReplicasMismatch
      annotations:
        summary: StatefulSet has not matched the expected number of replicas.
        runbook_url: {{ $values.defaults.runbookUrl }}#alert-name-kubestatefulsetreplicasmismatch
        description: >-
          StatefulSet {{`{{`}} $labels.namespace {{`}}`}}/{{`{{`}}
          $labels.statefulset {{`}}`}} has not matched the expected number of
          replicas for longer than 15 minutes.
      expr: |-
        (
          kube_statefulset_status_replicas_ready{ {{- $statefulsetSelector -}} }
            !=
          kube_statefulset_status_replicas{ {{- $statefulsetSelector -}} }
        ) and (
          changes(kube_statefulset_status_replicas_updated{ {{- $statefulsetSelector -}} }[5m])
            ==
          0
        )
      for: {{ .for }}
      labels:
        severity: {{ .severity }}
        {{- if $values.defaults.additionalRuleLabels }}
        {{ toYaml $values.defaults.additionalRuleLabels | nindent 8 }}
        {{- end }}
    {{- end }}

    {{ with .Values.containerRules.KubeStatefulSetGenerationMismatch -}}
    - alert: KubeStatefulSetGenerationMismatch
      annotations:
        summary: StatefulSet generation mismatch due to possible roll-back
        runbook_url: {{ $values.defaults.runbookUrl }}#alert-name-kubestatefulsetgenerationmismatch
        description: >-
          StatefulSet generation for {{`{{`}} $labels.namespace
          {{`}}`}}/{{`{{`}} $labels.statefulset {{`}}`}} does not match, this
          indicates that the StatefulSet has failed but has not been rolled
          back.
      expr: |-
        kube_statefulset_status_observed_generation{ {{- $statefulsetSelector -}} }
          !=
        kube_statefulset_metadata_generation{ {{- $statefulsetSelector -}} }
      for: {{ .for }}
      labels:
        severity: {{ .severity }}
        {{- if $values.defaults.additionalRuleLabels }}
        {{ toYaml $values.defaults.additionalRuleLabels | nindent 8 }}
        {{- end }}
    {{- end }}

    {{ with .Values.containerRules.KubeStatefulSetUpdateNotRolledOut -}}
    - alert: KubeStatefulSetUpdateNotRolledOut
      annotations:
        summary: StatefulSet update has not been rolled out.
        runbook_url: {{ $values.defaults.runbookUrl }}#alert-name-kubestatefulsetupdatenotrolledout
        description: >-
          StatefulSet {{`{{`}} $labels.namespace {{`}}`}}/{{`{{`}}
          $labels.statefulset {{`}}`}} update has not been rolled out.
      expr: |-
        (
          max without (revision) (
            kube_statefulset_status_current_revision{ {{- $statefulsetSelector -}} }
              unless
            kube_statefulset_status_update_revision{ {{- $statefulsetSelector -}} }
          )
            *
          (
            kube_statefulset_replicas{ {{- $statefulsetSelector -}} }
              !=
            kube_statefulset_status_replicas_updated{ {{- $statefulsetSelector -}} }
          )
        )  and (
          changes(kube_statefulset_status_replicas_updated{ {{- $statefulsetSelector -}} }[5m])
            ==
          0
        )
      for: {{ .for }}
      labels:
        severity: {{ .severity }}
        {{- if $values.defaults.additionalRuleLabels }}
        {{ toYaml $values.defaults.additionalRuleLabels | nindent 8 }}
        {{- end }}
    {{- end }}

    {{ with .Values.containerRules.KubeDaemonSetRolloutStuck -}}
    - alert: KubeDaemonSetRolloutStuck
      annotations:
        summary: DaemonSet rollout is stuck.
        runbook_url: {{ $values.defaults.runbookUrl }}#alert-name-kubedaemonsetrolloutstuck
        description: >-
          DaemonSet {{`{{`}} $labels.namespace {{`}}`}}/{{`{{`}}
          $labels.daemonset {{`}}`}} has not finished or progressed for at
          least {{ .for }}.
      expr: |-
        (
          (
            kube_daemonset_status_current_number_scheduled{ {{- $daemonsetSelector -}} }
             !=
            kube_daemonset_status_desired_number_scheduled{ {{- $daemonsetSelector -}} }
          ) or (
            kube_daemonset_status_number_misscheduled{ {{- $daemonsetSelector -}} }
             !=
            0
          ) or (
            kube_daemonset_updated_number_scheduled{ {{- $daemonsetSelector -}} }
             !=
            kube_daemonset_status_desired_number_scheduled{ {{- $daemonsetSelector -}} }
          ) or (
            kube_daemonset_status_number_available{ {{- $daemonsetSelector -}} }
             !=
            kube_daemonset_status_desired_number_scheduled{ {{- $daemonsetSelector -}} }
          )
        ) and (
          changes(kube_daemonset_updated_number_scheduled{ {{- $daemonsetSelector -}} }[5m])
            ==
          0
        )
      for: {{ .for }}
      labels:
        severity: {{ .severity }}
        {{- if $values.defaults.additionalRuleLabels }}
        {{ toYaml $values.defaults.additionalRuleLabels | nindent 8 }}
        {{- end }}
    {{- end }}

    {{ with .Values.containerRules.KubeDaemonSetNotScheduled -}}
    - alert: KubeDaemonSetNotScheduled
      annotations:
        summary: DaemonSet pods are not scheduled.
        runbook_url: {{ $values.defaults.runbookUrl }}#alert-name-kubedaemonsetnotscheduled
        description: >-
          '{{`{{`}} $value {{`}}`}} Pods of DaemonSet {{`{{`}}
          $labels.namespace {{`}}`}}/{{`{{`}} $labels.daemonset {{`}}`}} are
          not scheduled.'
      expr: |-
        kube_daemonset_status_desired_number_scheduled{ {{- $daemonsetSelector -}} }
          -
        kube_daemonset_status_current_number_scheduled{ {{- $daemonsetSelector -}} } > 0
      for: 10m
      labels:
        severity: {{ .severity }}
        {{- if $values.defaults.additionalRuleLabels }}
        {{ toYaml $values.defaults.additionalRuleLabels | nindent 8 }}
        {{- end }}
    {{- end }}

    {{ with .Values.containerRules.KubeDaemonSetMisScheduled -}}
    - alert: KubeDaemonSetMisScheduled
      annotations:
        summary: DaemonSet pods are misscheduled.
        runbook_url: {{ $values.defaults.runbookUrl }}#alert-name-kubedaemonsetmisscheduled
        description: >-
          '{{`{{`}} $value {{`}}`}} Pods of DaemonSet {{`{{`}}
          $labels.namespace {{`}}`}}/{{`{{`}} $labels.daemonset {{`}}`}} are
          running where they are not supposed to run.'
      expr: kube_daemonset_status_number_misscheduled{ {{- $daemonsetSelector -}} } > 0
      for: {{ .for }}
      labels:
        severity: {{ .severity }}
        {{- if $values.defaults.additionalRuleLabels }}
        {{ toYaml $values.defaults.additionalRuleLabels | nindent 8 }}
        {{- end }}
    {{- end }}

    {{ with .Values.containerRules.KubeJobCompletion -}}
    - alert: KubeJobCompletion
      annotations:
        summary: Job did not complete in time
        runbook_url: {{ $values.defaults.runbookUrl }}#alert-name-kubejobcompletion
        description: >-
          Job {{`{{`}} $labels.namespace {{`}}`}}/{{`{{`}} $labels.job_name
          {{`}}`}} is taking more than 12 hours to complete.
      expr: |-
        kube_job_spec_completions{ {{- $jobSelector -}} }
          -
        kube_job_status_succeeded{ {{- $jobSelector -}} }
          > 0
      for: {{ .for }}
      labels:
        severity: {{ .severity }}
        {{- if $values.defaults.additionalRuleLabels }}
        {{ toYaml $values.defaults.additionalRuleLabels | nindent 8 }}
        {{- end }}
    {{- end }}

    {{ with .Values.containerRules.KubeJobFailed -}}
    - alert: KubeJobFailed
      annotations:
        summary: Job failed to complete.
        runbook_url: {{ $values.defaults.runbookUrl }}#alert-name-kubejobfailed
        description: >-
          Job {{`{{`}} $labels.namespace {{`}}`}}/{{`{{`}} $labels.job_name
          {{`}}`}} failed to complete. Removing failed job after investigation
          should clear this alert.
      expr: kube_job_failed{ {{- $jobSelector -}} }  > 0
      for: {{ .for }}
      labels:
        severity: {{ .severity }}
        {{- if $values.defaults.additionalRuleLabels }}
        {{ toYaml $values.defaults.additionalRuleLabels | nindent 8 }}
        {{- end }}
    {{- end }}

    {{ with .Values.containerRules.KubeHpaReplicasMismatch -}}
    - alert: KubeHpaReplicasMismatch
      annotations:
        summary: HPA has not matched descired number of replicas.
        runbook_url: {{ $values.defaults.runbookUrl }}#alert-name-kubehpareplicasmismatch
        description: >-
          HPA {{`{{`}} $labels.namespace {{`}}`}}/{{`{{`}} $labels.hpa {{`}}`}}
          has not matched the desired number of replicas for longer than 15
          minutes.
      expr: |-
        (
         kube_horizontalpodautoscaler_status_desired_replicas{ {{- $hpaSelector -}} }
          !=
         kube_horizontalpodautoscaler_status_current_replicas{ {{- $hpaSelector -}} }
        )
          and

        (
         kube_horizontalpodautoscaler_status_current_replicas{ {{- $hpaSelector -}} }
          >
         kube_horizontalpodautoscaler_spec_min_replicas{ {{- $hpaSelector -}} }
        )

          and

        (
         kube_horizontalpodautoscaler_status_current_replicas{ {{- $hpaSelector -}} }
          <
         kube_horizontalpodautoscaler_spec_max_replicas{ {{- $hpaSelector -}} }
        )

          and

        changes(kube_horizontalpodautoscaler_status_current_replicas[15m]) == 0

      for: {{ .for }}
      labels:
        severity: {{ .severity }}
        {{- if $values.defaults.additionalRuleLabels }}
        {{ toYaml $values.defaults.additionalRuleLabels | nindent 8 }}
        {{- end }}
    {{- end }}

    {{ with .Values.containerRules.KubeHpaMaxedOut -}}
    - alert: KubeHpaMaxedOut
      annotations:
        summary: HPA is running at max replicas
        runbook_url: {{ $values.defaults.runbookUrl }}#alert-name-kubehpamaxedout
        description: >-
          HPA {{`{{`}} $labels.namespace {{`}}`}}/{{`{{`}} $labels.hpa {{`}}`}}
          has been running at max replicas for longer than 15 minutes.
      expr: |-
        kube_horizontalpodautoscaler_status_current_replicas{ {{- $hpaSelector -}} }
          ==
        kube_horizontalpodautoscaler_spec_max_replicas{ {{- $hpaSelector -}} }
      for: {{ .for }}
      labels:
        severity: {{ .severity }}
        {{- if $values.defaults.additionalRuleLabels }}
        {{ toYaml $values.defaults.additionalRuleLabels | nindent 8 }}
        {{- end }}
    {{- end }}

  - name: {{ .Release.Name }}.{{ .Release.Namespace }}.alertRulesSelectorsValidity
    rules:
    {{ with .Values.containerRules.AlertRulesSelectorsValidity -}}
    {{- if .resources.daemonsets.enabled }}
    - alert: DaemonsetSelectorValidity
      annotations:
        summary: DaemonsetSelector for prometheus-alerts is invalid
        runbook_url: {{ $values.defaults.runbookUrl }}#Alert-Rules-Selectors-Validity
        description: >-
          The DaemonsetSelector used for daemonset level alerts did not return any data.
          Please check the DaemonsetSelector applied in your prometheus-alerts chart
          is correct to ensure you are properly selecting your daemonsets so that you
          will be alerted for daemonset issues. The current selector is
          `{daemonsetSelector="{{ $daemonsetSelector }}"}`.
      expr: |-
        (
          count(
            kube_daemonset_labels{
              {{ $daemonsetSelector }}
            }
          ) or on() vector(0)
        ) == 0
      for: {{ .for }}
      labels:
        severity: {{ .severity }}
        {{- if $values.defaults.additionalRuleLabels }}
        {{ toYaml $values.defaults.additionalRuleLabels | nindent 8 }}
        {{- end }}
    {{- end }}
    {{- if .resources.deployments.enabled }}
    - alert: DeploymentSelectorValidity
      annotations:
        summary: DeploymentSelector for prometheus-alerts is invalid
        runbook_url: {{ $values.defaults.runbookUrl }}#Alert-Rules-Selectors-Validity
        description: >-
          The DeploymentSelector used for deployment level alerts did not return any data.
          Please check the DeploymentSelector applied in your prometheus-alerts chart
          is correct to ensure you are properly selecting your deployments so that you
          will be alerted for deployment issues. The current selector is
          `{deploymentSelector="{{ $deploymentSelector }}"}`.
      expr: |-
        (
          count(
            kube_deployment_labels{
              {{ $deploymentSelector }}
            }
          ) or on() vector(0)
        ) == 0
      for: {{ .for }}
      labels:
        severity: {{ .severity }}
        {{- if $values.defaults.additionalRuleLabels }}
        {{ toYaml $values.defaults.additionalRuleLabels | nindent 8 }}
        {{- end }}
    {{- end }}
    {{- if .resources.pods.enabled }}
    - alert: PodSelectorValidity
      annotations:
        summary: PodSelector for prometheus-alerts is invalid
        runbook_url: {{ $values.defaults.runbookUrl }}#Alert-Rules-Selectors-Validity
        description: >-
          The PodSelector used for pod level alerts did not return any data.
          Please check the PodSelector applied in your prometheus-alerts chart
          is correct to ensure you are properly selecting your pods so that you
          will be alerted for pod issues. The current selector is
          `{podSelector="{{ $podSelector }}", namespaceSelector="{{ $namespaceSelector }}"}`.
      expr: |-
        (
          count(
            kube_pod_info{
              {{ $podSelector }},
              {{ $namespaceSelector }}
            }
          ) or on() vector(0)
        ) == 0
      for: {{ .for }}
      labels:
        severity: {{ .severity }}
        {{- if $values.defaults.additionalRuleLabels }}
        {{ toYaml $values.defaults.additionalRuleLabels | nindent 8 }}
        {{- end }}
    {{- end }}
    {{- if .resources.jobs.enabled }}
    - alert: JobSelectorValidity
      annotations:
        summary: JobSelector for prometheus-alerts is invalid
        runbook_url: {{ $values.defaults.runbookUrl }}#Alert-Rules-Selectors-Validity
        description: >-
          The JobSelector used for job level alerts did not return any data.
          Please check the JobSelector applied in your prometheus-alerts chart
          is correct to ensure you are properly selecting your jobs so that you
          will be alerted for job issues. The current selector is
          `{jobSelector="{{ $jobSelector }}"}`.
      expr: |-
        (
          count(
            kube_job_info{
              {{ $jobSelector }}
            }
          ) or on() vector(0)
        ) == 0
      for: {{ .for }}
      labels:
        severity: {{ .severity }}
        {{- if $values.defaults.additionalRuleLabels }}
        {{ toYaml $values.defaults.additionalRuleLabels | nindent 8 }}
        {{- end }}
    {{- end }}
    {{- if .resources.hpas.enabled }}
    - alert: HpaSelectorValidity
      annotations:
        summary: HpaSelector for prometheus-alerts is invalid
        runbook_url: {{ $values.defaults.runbookUrl }}#Alert-Rules-Selectors-Validity
        description: >-
          The HpaSelector used for hpa level alerts did not return any data.
          Please check the HpaSelector applied in your prometheus-alerts chart
          is correct to ensure you are properly selecting your hpas so that you
          will be alerted for hpa issues. The current selector is
          `{hpaSelector="{{ $hpaSelector }}"}`.
      expr: |-
        (
          count(
            kube_horizontalpodautoscaler_info{
              {{ $hpaSelector }}
            }
          ) or on() vector(0)
        ) == 0
      for: {{ .for }}
      labels:
        severity: {{ .severity }}
        {{- if $values.defaults.additionalRuleLabels }}
        {{ toYaml $values.defaults.additionalRuleLabels | nindent 8 }}
        {{- end }}
    {{- end }}
    {{- if .resources.statefulsets.enabled }}
    - alert: StatefulsetSelectorValidity
      annotations:
        summary: StatefulsetSelector for prometheus-alerts is invalid
        runbook_url: {{ $values.defaults.runbookUrl }}#Alert-Rules-Selectors-Validity
        description: >-
          The StatefulsetSelector used for statefulset level alerts did not return any data.
          Please check the StatefulsetSelector applied in your prometheus-alerts chart
          is correct to ensure you are properly selecting your statefulsets so that you
          will be alerted for statefulset issues. The current selector is
          `{statefulsetSelector="{{ $statefulsetSelector }}"}`.
      expr: |-
        (
          count(
            kube_statefulset_created{
              {{ $statefulsetSelector }}
            }
          ) or on() vector(0)
        ) == 0
      for: {{ .for }}
      labels:
        severity: {{ .severity }}
        {{- if $values.defaults.additionalRuleLabels }}
        {{ toYaml $values.defaults.additionalRuleLabels | nindent 8 }}
        {{- end }}
    {{- end }}
    {{- end }}

{{- end }}
