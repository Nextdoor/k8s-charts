# Default values for deploying a flink job cluster with the word counting app

image:
  # -- (String) The Flink image name and repository
  repository: flink
  # -- (String) The Flink image tag
  tag: 1.13.1

# -- (String) The name of the flink cluster
fullnameOverride: word-counting-cluster

# -- Environment variables shared by all containers
envVars:
  # Prometheus reporter jar to be loaded by flink
  - name: HADOOP_CLASSPATH
    value: /opt/flink/opt/flink-metrics-prometheus-1.9.3.jar

# -- The logging configuration, a string-to-string map that becomes the ConfigMap mounted at /opt/flink/conf
logConfig:
  "log4j-console.properties": |
    rootLogger.level = INFO
    rootLogger.appenderRef.file.ref = LogFile
    rootLogger.appenderRef.console.ref = LogConsole
    appender.file.name = LogFile
    appender.file.type = File
    appender.file.append = false
    appender.file.fileName = ${sys:log.file}
    appender.file.layout.type = PatternLayout
    appender.file.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n
    appender.console.name = LogConsole
    appender.console.type = CONSOLE
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n
    logger.akka.name = akka
    logger.akka.level = INFO
    logger.kafka.name= org.apache.kafka
    logger.kafka.level = INFO
    logger.hadoop.name = org.apache.hadoop
    logger.hadoop.level = INFO
    logger.zookeeper.name = org.apache.zookeeper
    logger.zookeeper.level = INFO
    logger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline
    logger.netty.level = OFF
  "logback-console.xml": |
    <configuration>
      <appender name="console" class="ch.qos.logback.core.ConsoleAppender">
        <encoder>
          <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{60} %X{sourceThread} - %msg%n</pattern>
        </encoder>
      </appender>
      <appender name="file" class="ch.qos.logback.core.FileAppender">
        <file>${log.file}</file>
        <append>false</append>
        <encoder>
          <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{60} %X{sourceThread} - %msg%n</pattern>
        </encoder>
      </appender>
      <root level="INFO">
        <appender-ref ref="console"/>
        <appender-ref ref="file"/>
      </root>
      <logger name="akka" level="INFO" />
      <logger name="org.apache.kafka" level="INFO" />
      <logger name="org.apache.hadoop" level="INFO" />
      <logger name="org.apache.zookeeper" level="INFO" />
      <logger name="org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline" level="ERROR" />
    </configuration>

jobManager:
  # -- (String) Access scope of the JobManager service. enum("Cluster", "VPC", "External", "NodePort", "Headless")
  accessScope: Cluster
  # -- (`int`) Ports that JobManager listening on
  ports:
    ui: 8081
  # -- Prometheus metrics ports for jobManager
  metrics:
    enabled: true
    extraPorts:
      - name: prom
        containerPort: 9249
  # -- Compute resources required by JobManager container
  resources:
    requests:
      memory: "1000Mi"
      cpu: "100m"
    limits:
      memory: 1400Mi
      cpu: "2"  # flinkcluster controller needs this quoted

taskManager:
  # -- (`int`) The number of TaskManager replicas
  replicas: 1

  # -- Prometheus metrics ports for taskManager
  metrics:
    enabled: true
    extraPorts:
      - name: prom
        containerPort: 9249
        protocol: TCP
  # -- Compute resources required by TaskManager containers
  resources:
    requests:
      memory: "1000Mi"
      cpu: "100m"
    limits:
      memory: 1500Mi
      cpu: "2"  # flinkcluster controller needs this quoted
  # -- Allow flink user to read volumes
  securityContext:
    fsGroup: 9999
    runAsGroup: 9999
    runAsNonRoot: true
    runAsUser: 9999

job:
  # -- (String) JAR file of the job
  jarFile: ./examples/streaming/WordCount.jar
  # -- (String) Java class name of the job
  className: org.apache.flink.streaming.examples.wordcount.WordCount
  # -- (List) Command-line args of the job
  args: ["--input", "./README.txt", "--output", "./OUTPUT.txt"]
  # -- (`int`) Parallelism of the job
  parallelism: 1
  # -- (`int`) Automatically take a savepoint to the savepointsDir in this given interval
  autoSavepointSeconds: 30
  # -- (String) Directory to store automatically taken savepoints
  savepointsDir: /savepoint
  # -- (String) Restart policy when the job fails, enum("Never", "FromSavepointOnFailure")
  restartPolicy: FromSavepointOnFailure
  # -- The action to take after job finishes enum("KeepCluster", "DeleteCluster", "DeleteTaskManager")
  cleanupPolicy:
    afterJobFails: KeepCluster
    afterJobCancelled: KeepCluster
    afterJobSucceeds: KeepCluster
  # -- Init containers of the Job pod. It can be used to download a remote job jar to your job pod.
  # It is only needed if you have no other way to download your job files into the Flink job cluster.
  initContainers:
    enabled: false

# -- (`Map`) Flink properties which are appened to flink-conf.yaml
flinkProperties:
  execution.checkpointing.interval: 10min
  execution.checkpointing.mode: EXACTLY_ONCE
  restart-strategy: exponential-delay
  restart-strategy.exponential-delay.backoff-multiplier: "2.0"
  state.checkpoints.dir: file:/savepoint/
  taskmanager.numberOfTaskSlots: "1"
  kubernetes.cluster-id: "{{ .Values.fullnameOverride }}"
  kubernetes.namespace: "{{ .Release.Namespace }}"
  high-availability: org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory
  high-availability.storageDir: file:/savepoint/
  # metrics reporter "PrometheusReporter"
  # visit https://ci.apache.org/projects/flink/flink-docs-stable/monitoring/metrics.html#prometheus-orgapacheflinkmetricsprometheusprometheusreporter
  # for more information
  metrics.reporters: prom
  metrics.reporter.prom.class: org.apache.flink.metrics.prometheus.PrometheusReporter

# -- Extra Labels to be added to pod
podLabels:
  sidecar.istio.io/inject: "false"

# -- podMonitor for metrics - you need the Prometheus-Operator and its CRDs up and running in order to use PodMonitor.
podMonitor:
  enabled: true
  podTargetLabels:
    - cluster
    - component

  # include the podMonitorSelectorLabel which you have set in your prometheus-operator
  # set podMonitorSelectorLabels {} if your prometheus-operator is set to collect all podMonitors
  podMonitorSelectorLabels:
    prometheus: cluster-metrics

  selector:
    matchLabels:
      app: flink

  podMetricsEndpoints:
  - port: prom

serviceAccount:
  # -- (Boolean) Specifies whether a service account should be created
  create: true
  # -- (String) The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

# -- Configuration of the PersistentVolume for storing savepoints.
pvc:
  storageClassName: efs
  storage: 1Gi

# -- Configuration of the automatic savepoints
savepoints:
  # -- (String) The mount path of the savepoint volume
  savepointDir: "/savepoint"
  # -- (Boolean) Automatically creates a volume and mount the volume on task manager and job manager pods
  enabled: true

alerts:
  # -- (Boolean) Specifies whether to create the PrometheusRule for this flink cluster
  enabled: true
  # -- (String) Severity of the alerts
  severity: info
  # -- (`int`) The number of job restarts before alerting
  restartsLimit: 2

defaults:
  # -- (String) Runbook URL for the Prometheus alerts
  runbookUrl: https://github.com/Nextdoor/k8s-charts/blob/main/charts/flink-job/runbook.md
