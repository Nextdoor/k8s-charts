{{- $targetNamespace := .Release.Namespace }}
{{- $runbookUrl      := required "Values.runbookUrl can not be blank!" .Values.runbookUrl }}
{{- $appName         := include "nd-common.fullname" . }}
{{- $podName         := trunc 56 $appName | printf "%s.*" }}
{{- if .Values.prometheusRules.enabled }}
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: {{ .Release.Name }}-{{ .Chart.Name }}-rules
  labels:
    {{- include "nd-common.labels" . | nindent 4 }}
spec:
  groups:
  - name: {{ .Release.Namespace }}.{{ .Release.Name }}.{{ .Chart.Name }}.PodRules
    rules:

    {{- with .Values.prometheusRules.PodContainerTerminated }}
    - alert: PodContainerTerminated
      annotations:
        summary: {{`Pod {{$labels.pod}} in namespace {{$labels.namespace}} in error status`}}
        runbook_url: {{ $runbookUrl }}#kube-pod-container-terminated
        description: >-
          Pod {{`{{$labels.pod}}`}} in namespace {{`{{$labels.namespace}}`}}
          has a container that has been terminated ({{`{{ $value }}`}} times) due to
          {{`{{$labels.reason}}`}} in the last {{ .for }}.
      expr: |-
        sum by (container, namespace, pod, reason) (
          sum_over_time(
            kube_pod_container_status_terminated_reason{
              reason=~"{{ join "|" .reasons }}",
              namespace="{{ $targetNamespace }}",
              pod=~"{{ $podName }}"
            }[{{ .over }}]
          )
        ) > {{ .threshold }}
      for: {{ .for }}
      labels:
        severity: {{ .severity }}
        {{- with $.Values.prometheusRules.additionalRuleLabels }}
        {{ toYaml . | nindent 8 }}
        {{- end }}
    {{- end }}

    {{- with .Values.prometheusRules.PodCrashLoopBackOff }}
    - alert: PodCrashLoopBackOff
      annotations:
        summary: >-
          {{`Container inside pod {{ $labels.pod }} is crash looping`}}
        runbook_url: {{ $runbookUrl }}#alert-name-kubepodcrashlooping
        description: >-
          {{`Container {{ $labels.container }} within pod {{ $labels.pod }} is in
          a {{ $labels.reason }} state. Investigate because it indicates that
          the Pod is unable to become fully healthy.`}}
      expr: |-
        sum by(namespace, pod, container, reason) (
          kube_pod_container_status_waiting_reason{
            job="kube-state-metrics",
            reason="CrashLoopBackOff",
            namespace=~"{{ $targetNamespace }}",
            pod=~"{{ $podName }}"
          }
        ) > 0
      for: {{ .for }}
      labels:
        severity: {{ .severity }}
        {{- with $.Values.prometheusRules.additionalRuleLabels }}
        {{ toYaml . | nindent 8 }}
        {{- end }}
    {{- end }}

    {{- with .Values.prometheusRules.PodNotReady }}
    - alert: PodNotReady
      annotations:
        summary: >-
          {{`{{ $labels.pod }}`}} has been in a non-ready state for more than {{ .for }}
        runbook_url: {{ $runbookUrl }}#alert-name-kubepodnotready
        description: >-
          Pod {{`{{ $labels.pod }}`}} (namespace: {{`{{ $labels.namespace }}`}})
          has been in a non-ready state for longer than {{ .for }}.
      expr: |-
        sum by (namespace, pod) (
          max by(namespace, pod) (
            kube_pod_status_phase{
              job="kube-state-metrics",
              namespace=~"{{ $targetNamespace }}",
              phase=~"Pending|Unknown",
              pod=~"{{ $podName }}"
            }
          ) * on(namespace, pod) group_left(owner_kind) topk by(namespace, pod) (
            1, max by(namespace, pod, owner_kind) (kube_pod_owner{owner_kind!="Job"})
          )
        ) > 0
      for: {{ .for }}
      labels:
        severity: {{ .severity }}
        {{- with $.Values.prometheusRules.additionalRuleLabels }}
        {{ toYaml . | nindent 8 }}
        {{- end }}
    {{- end }}

  - name: {{ .Release.Namespace }}.{{ .Release.Name }}.{{ .Chart.Name }}.ContainerRules
    rules:

    {{- with .Values.prometheusRules.CPUThrottlingHigh }}
    - alert: CPUThrottlingHigh
      annotations:
        summary: >-
          {{`{{ $labels.pod }}`}} processes are experiencing elevated CPU throttling
        runbook_url: {{ $runbookUrl }}#CPUThrottlingHigh
        description: >-
          {{`{{ $value | humanizePercentage }}`}} throttling of CPU in
          namespace {{`{{ $labels.namespace }}`}} for container
          {{`{{ $labels.container }}`}} in pod {{`{{ $labels.pod }}`}}.
      expr: |-
        sum(
          increase(
            container_cpu_cfs_throttled_periods_total{
              container!="",
              namespace=~"{{ $targetNamespace }}",
              pod=~"{{ $podName }}"
            }[5m]
          )
        ) by (container, pod, namespace)

          /

        sum(
          increase(
            container_cpu_cfs_periods_total{}[5m]
          )
        ) by (container, pod, namespace)

        > ( {{ .threshold }} / 100 )
      for: {{ .for }}
      labels:
        severity: {{ .severity }}
        {{- with $.Values.prometheusRules.additionalRuleLabels }}
        {{ toYaml . | nindent 8 }}
        {{- end }}
    {{- end }}

    {{- with .Values.prometheusRules.ContainerWaiting }}
    - alert: ContainerWaiting
      annotations:
        summary: >-
          {{`{{ $labels.pod }}`}} container ({{`{{ $labels.container }}`}}) waiting longer than {{ .for }}
        runbook_url: {{ $runbookUrl }}#containerwaiting
        description: >-
          Container {{`{{ $labels.container }}`}} in pod {{`{{ $labels.pod }}`}}
          (namespace: {{`{{ $labels.namespace }}`}} has been in a waiting
          state for longer than 1 hour.
      expr: sum by (namespace, pod, container) (kube_pod_container_status_waiting_reason{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}) > 0
      for: {{ .for }}
      labels:
        severity: {{ .severity }}
        {{- with $.Values.prometheusRules.additionalRuleLabels }}
        {{ toYaml . | nindent 8 }}
        {{- end }}
    {{- end }}

  - name: {{ .Release.Namespace }}.{{ .Release.Name }}.{{ .Chart.Name }}.DeploymentRules
    rules:

    {{- with .Values.prometheusRules.DeploymentGenerationMismatch }}
    - alert: DeploymentGenerationMismatch
      annotations:
        summary: >-
          {{`{{ $labels.deployment }}`}} deployment generation mismatch due to possible roll-back
        runbook_url: {{ $runbookUrl }}#deploymentgenerationmismatch
        description: >-
          Deployment generation for {{`{{ $labels.namespace }}`}}/{{`{{ $labels.deployment }}`}}
          does not match, this indicates that the Deployment has failed but has not been rolled
          back.
      expr: |-
        sum by(namespace, deployment) (
          kube_deployment_status_observed_generation{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}", deployment="{{ $appName }}"}
            !=
          kube_deployment_metadata_generation{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}", deployment="{{ $appName }}"}
        )
      for: {{ .for }}
      labels:
        severity: {{ .severity }}
        {{- with $.Values.prometheusRules.additionalRuleLabels }}
        {{ toYaml . | nindent 8 }}
        {{- end }}
    {{- end }}

  {{- if .Values.autoscaling.enabled }}
  - name: {{ .Release.Namespace }}.{{ .Release.Name }}.{{ .Chart.Name }}.HorizontalPodAutoscalerRules
    rules:

    {{- with .Values.prometheusRules.HpaReplicasMismatch }}
    - alert: HpaReplicasMismatch
      annotations:
        summary: >-
          {{`{{ $labels.horizontalpodautoscaler }}`}} HPA has not matched descired number of replicas.
        runbook_url: {{ $runbookUrl }}#hpareplicasmismatch
        description: >-
          HPA {{`{{ $labels.namespace }}`}}/{{`{{ $labels.horizontalpodautoscaler }}`}}
          has not matched the desired number of replicas for longer than 15
          minutes.
      expr: |-
        sum by(namespace, horizontalpodautoscaler) (
          (kube_horizontalpodautoscaler_status_desired_replicas{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}", horizontalpodautoscaler="{{ $appName }}"}
            !=
          kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}", horizontalpodautoscaler="{{ $appName }}"})
            and
          (kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}", horizontalpodautoscaler="{{ $appName }}"}
            >
          kube_horizontalpodautoscaler_spec_min_replicas{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}", horizontalpodautoscaler="{{ $appName }}"})
            and
          (kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}", horizontalpodautoscaler="{{ $appName }}"}
            <
          kube_horizontalpodautoscaler_spec_max_replicas{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}", horizontalpodautoscaler="{{ $appName }}"})
            and
          changes(kube_horizontalpodautoscaler_status_current_replicas[15m]) == 0
        )
      for: {{ .for }}
      labels:
        severity: {{ .severity }}
        {{- with $.Values.prometheusRules.additionalRuleLabels }}
        {{ toYaml . | nindent 8 }}
        {{- end }}
    {{- end }}

    {{- with .Values.prometheusRules.HpaMaxedOut }}
    - alert: HpaMaxedOut
      annotations:
        summary: >-
          {{`{{ $labels.horizontalpodautoscaler }}`}} HPA is running at max replicas
        runbook_url: {{ $runbookUrl }}#hpamaxedout
        description: >-
          HPA {{`{{ $labels.namespace }}`}}/{{`{{ $labels.horizontalpodautoscaler }}`}}
          has been running at max replicas for longer than 15 minutes.
      expr: |-
        sum by(namespace, horizontalpodautoscaler) (
          kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}", horizontalpodautoscaler="{{ $appName }}"}
            ==
          kube_horizontalpodautoscaler_spec_max_replicas{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}", horizontalpodautoscaler="{{ $appName }}"}
        )
      for: {{ .for }}
      labels:
        severity: {{ .severity }}
        {{- with $.Values.prometheusRules.additionalRuleLabels }}
        {{ toYaml . | nindent 8 }}
        {{- end }}
    {{- end }}
  {{- end }}

{{- end }}
