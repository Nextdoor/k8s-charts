# Default monitor values
serviceName:    # Default pagerduty service name for the alerts, will turn to a tag for alerts - if not provided, the .Release.name will be used by default
team:           # Default pagerduty team name for the alerts, will turn to a tag for alerts - if not provided, the tag will not be added

# Placeholder for default datadog monitors
monitors: {}
#   failed-pods: # Required unique monitor resource name(needed to allow value overrides and used a datadog monitor resource name)
#     name: "[kubernetes] Monitor Kubernetes Failed Pods in Namespaces" # Required monitor name
#     message: "More than ten pods are failing in ({{kube_cluster_name.name}} cluster). \n The threshold of ten pods varies depending on your infrastructure. Change the threshold to suit your needs." # Required monitor message
#     priority: "2" # Optional alert severity/priority
#     query: "change(avg(last_5m),last_5m):sum:kubernetes_state.pod.status_phase{phase:failed} by {kube_cluster_name,kube_namespace} > 10" # Required alert query
#     type: "query alert"  # Optional, defaults to 'query alert'.
# // The type of monitor chosen from:
# // - anomaly: `query alert`
# // - APM: `query alert` or `trace-analytics alert`
# // - composite: `composite`
# // - custom: `service check`
# // - forecast: `query alert`
# // - host: `service check`
# // - integration: `query alert` or `service check`
# // - live process: `process alert`
# // - logs: `log alert`
# // - metric: `query alert`
# // - network: `service check`
# // - outlier: `query alert`
# // - process: `service check`
# // - rum: `rum alert`
# // - SLO: `slo alert`
# // - watchdog: `event-v2 alert`
# // - event-v2: `event-v2 alert`
# // - audit: `audit alert`
# // - error-tracking: `error-tracking alert`
# // - database-monitoring: `database-monitoring alert`
# // - network-performance: `network-performance alert`
#     tags:  # Optional list of tags (will be added on top of the default tags:service, team, namespace, cluster, app_env, app_group)
#       - 'tagname:tagvalue'
#     options: # Optional monitor parameters
#       thresholds: # Optional alert thresholds
#         critical: "1" # Optional critical threshold
#         warning: "0.28" # Optional warning threshold, critical threshold will be required if warning is specified
#       evaluationDelay: 300 # Time in seconds to wait before evaluating the monitor
#       groupbySimpleMonitor: false # Whether or not to group by simple monitor, triggers a single alert or multiple alerts when any group breaches the threshold
#       includeTags: false # A Boolean indicating whether notifications from this monitor automatically inserts its triggering tags into the title.
#       newGroupDelay: 300 # Time (in seconds) to allow a host to boot and applications to fully start before starting the evaluation
#       notifyNoData: false # A Boolean indicating whether this monitor notifies when data stops reporting.
#       noDataTimeframe: 30 # The number of minutes before a monitor notifies after data stops reporting. Datadog recommends at least 2x the monitor timeframe for metric alerts or 2 minutes for service checks. If omitted, 2x the evaluation timeframe is used for metric alerts, and 24 hours is used for service checks.
#       renotifyInterval: 0 # The number of minutes after the last notification before a monitor re-notifies on the current status.
#       renotifyOccurrences: 0 # The number of times re-notification messages should be sent on the current status at the provided re-notification interval.
#       renotifyStatus: [] # The types of statuses for which re-notification messages should be sent. Valid values are alert, warn, no data.
#       notifyBy: [] # List of labels indicating the granularity for a monitor to alert on. Only available for monitors with groupings.
#       requireFullWindow: false # A Boolean indicating whether this monitor requires full window of data before it will fire. We highly recommend you set this to false for sparse metrics, otherwise some evaluations are skipped.
#       thresholdWindows: # Threshold windows to finetune alerting
#         recoveryWindow: "10m" # Describes how long an anomalous metric must be normal before the alert recovers.
#         alertWindow: "5m" # Describes how long an anomalous metric must be anomalous before the alert fires.
